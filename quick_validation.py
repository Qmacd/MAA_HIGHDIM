#!/usr/bin/env python3
"""
è·¯å¾„æ˜ å°„å’Œè¾“å‡ºç»“æ„å¿«é€ŸéªŒè¯
"""

import os
import sys
import json
import pandas as pd
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_mock_output():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„å®Œæ•´è¾“å‡ºç»“æ„æ¥æµ‹è¯•"""
    logger.info("åˆ›å»ºæ¨¡æ‹Ÿçš„å®Œæ•´è¾“å‡ºç»“æ„...")
    
    asset_name = "Gold"
    base_dir = f"output/{asset_name}_processed"
    asset_dir = f"{base_dir}/{asset_name}"
    model_dir = f"{asset_dir}/concat_Baseline"
    training_logs_dir = f"{asset_dir}/training_logs"
    training_plots_dir = f"{asset_dir}/training_plots"
    
    # åˆ›å»ºç›®å½•ç»“æ„
    for dir_path in [base_dir, asset_dir, model_dir, training_logs_dir, training_plots_dir]:
        os.makedirs(dir_path, exist_ok=True)
        logger.info(f"  åˆ›å»ºç›®å½•: {dir_path}")
    
    # åˆ›å»ºé¢„æµ‹æ•°æ®
    predictions_data = pd.DataFrame({
        'datetime': pd.date_range('2020-01-01', periods=100, freq='D'),
        'close': range(100, 200),
        'open': range(99, 199),
        'high': range(101, 201),
        'low': range(98, 198),
        'volume': [1000] * 100,
        'True_Class': [0, 1, 2] * 33 + [0],
        'Predicted_Class': [0, 1, 2] * 33 + [1],
        'Predicted_Probability_0': [0.3] * 100,
        'Predicted_Probability_1': [0.4] * 100,
        'Predicted_Probability_2': [0.3] * 100,
    })
    
    predictions_file = f"{asset_dir}/predictions.csv"
    predictions_data.to_csv(predictions_file, index=False)
    logger.info(f"  åˆ›å»ºé¢„æµ‹æ–‡ä»¶: {predictions_file}")
    
    # åˆ›å»ºç»“æœæ–‡ä»¶
    results_data = pd.DataFrame({
        'metric': ['accuracy', 'precision', 'recall', 'f1_score', 'confusion_matrix'],
        'value': [0.85, 0.82, 0.88, 0.85, '[[10,2,1],[1,15,2],[0,1,12]]']
    })
    
    results_file = f"{asset_dir}/{asset_name}_results.csv"
    results_data.to_csv(results_file, index=False)
    logger.info(f"  åˆ›å»ºç»“æœæ–‡ä»¶: {results_file}")
    
    # åˆ›å»ºå¯è§†åŒ–æ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿï¼‰
    viz_files = [
        f"{asset_dir}/real_vs_predicted_scatter.png",
        f"{asset_dir}/real_vs_predicted_curve.png"
    ]
    
    for viz_file in viz_files:
        with open(viz_file, 'w') as f:
            f.write("Mock PNG content")
        logger.info(f"  åˆ›å»ºå¯è§†åŒ–æ–‡ä»¶: {viz_file}")
    
    # åˆ›å»ºè®­ç»ƒæ—¥å¿—
    training_log = {
        'experiment_name': f'{asset_name}_classification_concat_Baseline',
        'total_epochs': 5,
        'best_epoch': 4,
        'best_accuracy': 0.85,
        'final_loss': 0.32,
        'training_time': 120.5
    }
    
    log_file = f"{training_logs_dir}/{asset_name}_classification_concat_Baseline_training_log.json"
    with open(log_file, 'w') as f:
        json.dump(training_log, f, indent=2)
    logger.info(f"  åˆ›å»ºè®­ç»ƒæ—¥å¿—: {log_file}")
    
    # åˆ›å»ºè®­ç»ƒå›¾è¡¨æ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿï¼‰
    plot_files = [
        f"{training_plots_dir}/finetune_accuracy_train_val.png",
        f"{training_plots_dir}/finetune_loss_train_val.png", 
        f"{training_plots_dir}/finetune_training_curves.png",
        f"{training_plots_dir}/training_metrics.csv",
        f"{training_plots_dir}/training_summary.txt"
    ]
    
    for plot_file in plot_files:
        if plot_file.endswith('.csv'):
            pd.DataFrame({
                'epoch': range(1, 6),
                'train_loss': [0.8, 0.6, 0.5, 0.4, 0.32],
                'val_loss': [0.7, 0.55, 0.48, 0.42, 0.35],
                'train_acc': [0.6, 0.7, 0.75, 0.8, 0.85],
                'val_acc': [0.65, 0.72, 0.78, 0.82, 0.83]
            }).to_csv(plot_file, index=False)
        elif plot_file.endswith('.txt'):
            with open(plot_file, 'w') as f:
                f.write("Training Summary\n")
                f.write("Best epoch: 4\n")
                f.write("Best accuracy: 0.85\n")
        else:
            with open(plot_file, 'w') as f:
                f.write("Mock PNG content")
        logger.info(f"  åˆ›å»ºè®­ç»ƒå›¾è¡¨: {plot_file}")
    
    # åˆ›å»ºæ¨¡å‹æ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿï¼‰
    model_files = [
        f"{model_dir}/encoder.pt",
        f"{model_dir}/classifier.pt"
    ]
    
    for model_file in model_files:
        with open(model_file, 'w') as f:
            f.write("Mock PyTorch model content")
        logger.info(f"  åˆ›å»ºæ¨¡å‹æ–‡ä»¶: {model_file}")
    
    logger.info("âœ… æ¨¡æ‹Ÿè¾“å‡ºç»“æ„åˆ›å»ºå®Œæˆ")
    return predictions_file, results_file

def test_scanning():
    """æµ‹è¯•æ‰«æåŠŸèƒ½"""
    logger.info("æµ‹è¯•æ‰«æåŠŸèƒ½...")
    
    try:
        from complete_pipeline import PipelineConfig, BacktestRunner
        
        config = PipelineConfig()
        backtest_runner = BacktestRunner(config)
        
        experiments = backtest_runner.scan_experiment_results()
        
        if experiments:
            logger.info(f"âœ… æ‰«ææˆåŠŸï¼Œæ‰¾åˆ° {len(experiments)} ä¸ªå®éªŒ")
            for exp in experiments:
                logger.info(f"  å®éªŒ: {exp['asset_name']}/{exp['experiment_type']}/{exp['fusion_mode']}/{exp['pretrain_type']}")
                logger.info(f"    é¢„æµ‹æ–‡ä»¶: {exp['predictions_path']}")
                logger.info(f"    ç»“æœæ–‡ä»¶: {exp['results_path']}")
                
                # éªŒè¯æ–‡ä»¶å­˜åœ¨
                pred_ok = os.path.exists(exp['predictions_path'])
                result_ok = os.path.exists(exp['results_path'])
                logger.info(f"    æ–‡ä»¶çŠ¶æ€: é¢„æµ‹{'âœ…' if pred_ok else 'âŒ'} ç»“æœ{'âœ…' if result_ok else 'âŒ'}")
            
            return True, experiments
        else:
            logger.error("âŒ æ‰«æå¤±è´¥ï¼Œæœªæ‰¾åˆ°å®éªŒ")
            return False, []
    
    except Exception as e:
        logger.error(f"âŒ æ‰«æå¼‚å¸¸: {e}")
        return False, []

def analyze_output_structure():
    """åˆ†æè¾“å‡ºç»“æ„çš„å®Œæ•´æ€§"""
    logger.info("åˆ†æè¾“å‡ºç»“æ„å®Œæ•´æ€§...")
    
    output_dir = "output"
    if not os.path.exists(output_dir):
        logger.error("âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
        return False
    
    structure_analysis = {}
    
    for root, dirs, files in os.walk(output_dir):
        rel_path = os.path.relpath(root, output_dir)
        if rel_path == ".":
            rel_path = "output/"
        else:
            rel_path = f"output/{rel_path}/"
        
        structure_analysis[rel_path] = {
            'dirs': dirs,
            'files': files,
            'file_count': len(files)
        }
    
    logger.info("ğŸ“Š è¾“å‡ºç»“æ„åˆ†æ:")
    for path, info in structure_analysis.items():
        logger.info(f"  {path}")
        if info['files']:
            logger.info(f"    æ–‡ä»¶({info['file_count']}): {', '.join(info['files'][:3])}{'...' if len(info['files']) > 3 else ''}")
        if info['dirs']:
            logger.info(f"    å­ç›®å½•: {', '.join(info['dirs'])}")
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶ç±»å‹
    key_file_types = {
        '.csv': 0,
        '.png': 0,
        '.json': 0,
        '.pt': 0,
        '.txt': 0
    }
    
    for path, info in structure_analysis.items():
        for file in info['files']:
            for ext in key_file_types:
                if file.endswith(ext):
                    key_file_types[ext] += 1
    
    logger.info("ğŸ“ˆ æ–‡ä»¶ç±»å‹ç»Ÿè®¡:")
    for ext, count in key_file_types.items():
        logger.info(f"  {ext}: {count} ä¸ª")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ–‡ä»¶ç±»å‹
    expected_minimums = {'.csv': 2, '.png': 2, '.json': 1}  # è‡³å°‘éœ€è¦çš„æ–‡ä»¶æ•°
    all_good = True
    
    for ext, min_count in expected_minimums.items():
        if key_file_types[ext] < min_count:
            logger.warning(f"âš ï¸ {ext} æ–‡ä»¶æ•°é‡ä¸è¶³: {key_file_types[ext]} < {min_count}")
            all_good = False
        else:
            logger.info(f"âœ… {ext} æ–‡ä»¶æ•°é‡å……è¶³: {key_file_types[ext]} >= {min_count}")
    
    return all_good

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("è·¯å¾„æ˜ å°„å’Œè¾“å‡ºç»“æ„å¿«é€ŸéªŒè¯")
    print("=" * 60)
    
    # 1. åˆ›å»ºæ¨¡æ‹Ÿè¾“å‡º
    predictions_file, results_file = create_mock_output()
    
    # 2. åˆ†æè¾“å‡ºç»“æ„
    structure_ok = analyze_output_structure()
    
    # 3. æµ‹è¯•æ‰«æåŠŸèƒ½
    scan_ok, experiments = test_scanning()
    
    # 4. è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    print("éªŒè¯ç»“æœæ€»ç»“")
    print("=" * 60)
    
    results = [
        ("è¾“å‡ºç»“æ„åˆ›å»º", True),
        ("ç»“æ„å®Œæ•´æ€§åˆ†æ", structure_ok),
        ("æ‰«æåŠŸèƒ½æµ‹è¯•", scan_ok)
    ]
    
    success_count = sum(1 for name, success in results if success)
    total_count = len(results)
    
    for name, success in results:
        status = "âœ…" if success else "âŒ"
        print(f"{name}: {status}")
    
    print(f"\næ€»ä½“ç»“æœ: {success_count}/{total_count}")
    
    if success_count == total_count:
        print("\nğŸ‰ è·¯å¾„æ˜ å°„éªŒè¯æˆåŠŸï¼")
        print("\nå…³é”®å‘ç°:")
        print("âœ… è¾“å‡ºç»“æ„ï¼šå®Œæ•´ï¼ˆåŒ…å«é¢„æµ‹ã€ç»“æœã€å›¾è¡¨ã€æ—¥å¿—ã€æ¨¡å‹ï¼‰")
        print("âœ… æ‰«æé€»è¾‘ï¼šæ­£ç¡®è¯†åˆ«å®éªŒé…ç½®å’Œæ–‡ä»¶è·¯å¾„")
        print("âœ… æ–‡ä»¶ç»„ç»‡ï¼šç¬¦åˆmain1_maa.pyè¾“å‡º + complete_pipeline.pyæ‰«æé€»è¾‘")
        
        if experiments:
            print(f"\nğŸ“‹ å‘ç°çš„å®éªŒ:")
            for exp in experiments:
                print(f"  â€¢ {exp['asset_name']}: {exp['fusion_mode']}/{exp['pretrain_type']}")
    else:
        print("\nâŒ éƒ¨åˆ†éªŒè¯å¤±è´¥")
    
    return success_count == total_count

if __name__ == "__main__":
    main()
